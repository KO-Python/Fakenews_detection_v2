

import streamlit as st
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import pandas as pd
import os
from datetime import datetime
import dropbox

# âœ… ë“œë¡­ë°•ìŠ¤ Access Token
DROPBOX_ACCESS_TOKEN = 'sl.u.AF3XnPnBw8o3nWyPHw-Wluvb9gzy5VFeXWgn20xDaFp33HeMYJn63W5DRYOx71c3JSAhxVLbDdFCD8p6n7DPgSXVO8L4F9cqoQqV1ks18sRjEihcI1PnelcMqaCGULClWofoTJ-utzXHCe-ydGq5uH9O_FSeTiVdq07-ivBH70FTCazpLEp3xxKzFe2WqUXpao8oNU5gO-v4oMVezmC2YeAiv81c6aAoPQEMs2WVnq3n6wDGT95Gr7qCUdBvGgUjtk7ifPbafJ_X_ywNu7jK-9i6A2zMHuVp-8WKvpPWlV5j_IG3rAnqj_sky4a6F7D_yAv4cS0y_TMVKo_v1GXZd_7f3oBD2OfBxls0Kne4Pe3JeVRNJP_f59g8_VBXOPnpxJ-ESwwqU_SI_gVuWfUWk6bqLnP_oSNKTd1C__q4xb4wY9GNGkWTU9ci3NXpWli6cixEKxj-6J3JrvCXiJohrkmDsOMWdWN5yFaQj656YOHddE-OePwZi_IVV3tIg7qQgTh4TOfo68k3rKgptXY5MUcWzRyntDXb-Ry7ZewjNCJh6xvNxnYFilL3QpQpTkCrZ9DuXkofs5umnRR9wwv9ZM1kqdGAowyO8joGbtFLXsmCs43U79zBfBlA2bQ45sEb7RqEx7LWttwTKfefdyYjmpQQ__t11JOXQc_N7EbNh7OukEcL0TzO19GTlOBVFrJN2XOUGhqY41i0JSa1chHpfkb6zJQiCPI2qPKb19VNvuSH6VjQX57ZQRfYxi375fXkz-bbdX4FSmZoMjc9S0wIIsENlS5icStdKPV0papHdfapbKQS4fnkjWHAr70OL5z_H4eBsL7yDgsQmu_dfneUkyV_zcTyO8bLo09oGZwVdGfD1v3clR4uNv8bh8W2e98F0bplra6ySfx6CVSaaVjTfgQJD6R9a_y4ZVFMarD3uYpQctYgmrTd_4rtCo6auNnLXopdsq-ULJR8N8Fda1QaBooDrGnhwjyZHsmz9EPfsoR3ZHysiX7SxehYXIT8YQ5klCtP5gHe9M_4eupDjCoUGiqtv2yqpkpksPMuv28FZ75GuD7X0rIagCE_CA4flVML4L-eiT3W78zMfx3xtClrTbE6Ws8IS7YVQE2xRKIg-GR_jqSsPkRRP03bHyg_5x87kUAf8xtblAe28bgEgCUtPkCT3H3leW5iJqjVH5uDFkzwKw195BURkKFcu7bMlVSSprs0yPT15NUOyzt-dCtiy7uiaOdTnVFQYdFkcdli9QYyXGfXcQm2eefJZjUGD-R_EO8'

# âœ… ë“œë¡­ë°•ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)

# âœ… ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì ìš©)
@st.cache_resource
def load_model():
    model = BertForSequenceClassification.from_pretrained("kbs0035/my_fakenews_model")
    tokenizer = BertTokenizer.from_pretrained("kbs0035/my_fakenews_model")
    model.eval()
    return model, tokenizer

# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model, tokenizer = load_model()

# âœ… Streamlit UI êµ¬ì„±
st.title("í—ˆìœ„ì •ë³´ íƒì§€ AI ì„œë¹„ìŠ¤")

st.markdown("### ğŸ“¢ ì°¸ì—¬ ì•ˆë‚´")
st.info(
    "- ìµœê·¼ ì ‘í•œ ì •ë³´ ë˜ëŠ” ë‰´ìŠ¤ ì¤‘ì—ì„œ **íŠ¹ì • ì •ì¹˜ì¸ ë˜ëŠ” ì •ë‹¹ì— ëŒ€í•´ 'ì§„ì‹¤'ì´ë¼ê³  ë¯¿ëŠ” ë‚´ìš©**ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
"
    "- ì˜ˆì‹œ: â€˜â—‹â—‹â—‹ì´ â—‹â—‹ì„ ì¶”ì§„í–ˆë‹¤ëŠ” ë³´ë„ëŠ” ì‚¬ì‹¤ì´ë‹¤.â€™
"
    "- 1ê°œ ì´ìƒ, ìµœëŒ€ 3ê°œê¹Œì§€ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"
    "- ì…ë ¥ì´ ì™„ë£Œë˜ë©´ ìë™ ì €ì¥ë©ë‹ˆë‹¤."
)

user_id = st.text_input("ì°¸ì—¬ì½”ë“œ (ì „í™”ë²ˆí˜¸ ë 4ìë¦¬ ë˜ëŠ” ì„ì˜ 4ìë¦¬)")

# âœ… ê²€ìƒ‰ íšŸìˆ˜ ì¹´ìš´íŠ¸ (1~3ê°œ ì œí•œ)
if 'search_count' not in st.session_state:
    st.session_state['search_count'] = 0

st.write(f"í˜„ì¬ ì…ë ¥ íšŸìˆ˜: {st.session_state['search_count']} / 3 (ìµœëŒ€ 3ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥)")

# âœ… ê¸°ì‚¬ ì…ë ¥
st.subheader("ì •ë³´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”")
user_input = st.text_area("ë‚´ìš© ì…ë ¥", height=150)

# âœ… ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
if st.button("í—ˆìœ„ì •ë³´ íƒìƒ‰í•˜ê¸°"):
    if user_id.strip() == "" or user_input.strip() == "":
        st.warning("âš ï¸ ì°¸ì—¬ì½”ë“œì™€ ê¸°ì‚¬ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    elif st.session_state['search_count'] >= 3:
        st.warning("âš ï¸ ìµœëŒ€ 3ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(user_input, return_tensors="pt", max_length=128, truncation=True, padding="max_length")

        # ëª¨ë¸ ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            prediction = torch.argmax(logits, dim=1).item()
            probabilities = torch.softmax(logits, dim=1)
            confidence = probabilities[0][prediction].item() * 100

        # ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
        if prediction == 1:
            st.error(f"âŒ í—ˆìœ„ ì •ë³´ ê°€ëŠ¥ì„± ë†’ìŒ. (ì‹ ë¢°ë„: {confidence:.2f}%)")
            result_text = "í—ˆìœ„"
        else:
            st.success(f"âœ… ì§„ì‹¤ëœ ì •ë³´ ê°€ëŠ¥ì„± ë†’ìŒ. (ì‹ ë¢°ë„: {confidence:.2f}%)")
            result_text = "ì§„ì‹¤"

        # ë¡œê·¸ ì €ì¥
        log_entry = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'user_id': user_id,
            'search_count': st.session_state['search_count'] + 1,
            'user_input': user_input,
            'result': result_text,
            'confidence': round(confidence, 2)
        }

        log_file = 'search_log.csv'

        if os.path.exists(log_file):
            df_log = pd.read_csv(log_file)
            df_log = pd.concat([df_log, pd.DataFrame([log_entry])], ignore_index=True)
        else:
            df_log = pd.DataFrame([log_entry])

        df_log.to_csv(log_file, index=False)
        st.info("âœ… ê²€ìƒ‰ ë‚´ìš©ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # âœ… ë“œë¡­ë°•ìŠ¤ ì—…ë¡œë“œ
        try:
            with open("search_log.csv", "rb") as f:
                dbx.files_upload(f.read(), "/FakeNews/search_log.csv", mode=dropbox.files.WriteMode.overwrite)
            st.success("âœ… ì €ì¥ ì™„ë£Œ!")
        except Exception as e:
            st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

        # âœ… ê²€ìƒ‰ ì¹´ìš´íŠ¸ ì¦ê°€
        st.session_state['search_count'] += 1

# âœ… ê²€ìƒ‰ ì™„ë£Œ ì•ˆë‚´
if st.session_state['search_count'] == 3:
    st.success("ğŸ‰ 3ê°œ ì…ë ¥ ì™„ë£Œ! ê°ì‚¬í•©ë‹ˆë‹¤.")



